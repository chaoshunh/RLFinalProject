\documentclass[conference]{IEEEtran}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{url}
\usepackage[cmex10]{amsmath}
\usepackage{algorithmic}
\usepackage{array}
\usepackage{mdwmath}
\usepackage{mdwtab}
\usepackage{eqparbox}
\usepackage[font=footnotesize]{subfig}
\usepackage{fixltx2e}
\usepackage{setspace}

\begin{document}
\onehalfspacing
\title{A Reinforcement Learning Approach to Constrained Resource Allocation Problems}

\author{\IEEEauthorblockN{C. Vic Hu}
\IEEEauthorblockA{Department of Electrical \& Computer Engineering\\University of Texas at Austin}
}
\maketitle

\begin{abstract}
Resource allocation has been extensively studied in various fields such as wireless communication, intelligent traffic routing, industrial management, parallel architectures and distributed systems. Although there have been many efficient and powerful algorithms proposed in each domains, very few of them is able to address more generalized resource allocation problems on a higher level.
	
This paper proposes a generalized framework and makes three main contributions to solving constrained resource allocation problem using a modified reinforcement learning algorithm. First, we designed a general architecture, Constrained Resource Allocation Framework (CRAF), for the ease of generalized problem mapping and learning. Second, we defined four properties and three measurements to both quantitatively and qualitatively analyze how well an algorithm performs in CRAF. Finally, we developed three benchmark experiments to demonstrate how a reinforcement algorithm can successfully solve very different resource allocating problems with CRAF.
\end{abstract}

%Motivate and abstractly describe the problem you are addressing and how you are addressing it. What is the problem? Why is it important? What is your basic approach? A short discussion of how it fits into related work in the area is also desirable. Summarize the basic results and conclusions that you will present. 
\section{Introduction}
Resource allocation is an old and widely-solved problem in many fields, including electrical engineering, computer science, economics, management science and many more. It typically involves a fixed number of resource units to be distributed to a set of tasks over a period of time, such as landing aircraft scheduling, wireless communication routing, social security welfare and shared resources in parallel computer architectures. The problems of resource allocating are ubiquitous, but they all essentially boil down to one simple notion--based on a set of criteria, how many resource units should be allocated to which task first, and for how long.

To name a few remarkable research examples to solve this type of problems, Dresner and Stone proposed s reservation system for autonomous intersection management \cite{dresner2008} to allocate right of road for crossing traffic, Perkins and Royer presented a novel routing algorithm for ad-hoc on-demand mobile nodes management \cite{perkins1999}, and Foster et al. came up with a reservation and allocation architecture for heterogeneous resource management in computer network. While they all addressed an elegant solution to a specific domain, most of the techniques cannot be easily transferred to similar decision problems in other domains. In this paper, we focus on addressing exactly this issue and forming a general resource allocation framework that is suitable to be solved by a reinforcement learning method.

The first contribution of this paper is to form the Constrained Resource Allocation Framework (CRAF), in which we designed a generalized architecture to capture most of the resource allocation problems. In addition to the conventional first-come, first-served basis, we introduced the notion of constraints and queue propagation to reflect a more realistic setting and to relax more complicated systems into a single-frontier problem.

Secondly, we defined a collection of analysis criteria and evaluation methods to both qualitatively and quantitatively study how a reinforcement learning algorithm perform on our framework. Lastly, we proposed three benchmark problems to empirically demonstrate how CRAF applies to different resource allocation problems consistently and effectively.


\section{Background}

Unlike the Markov Decision Processes (MDPs) that most of the reinforcement learning algorithms are designed to solve, the notion of state representation, transition functions and reward functions in resource allocation problems such as aircraft landing scheduling can be very complex and challenging to define. Furthermore, the state space representing the entire global snapshot could be too large to be useful for effective learning. 

Instead of trying to model a resource allocation problem as MDP, we found it more intuitive to formalize it as a multi-armed bandit problem, which has been extensively researched in the field of reinforcement learning \cite{auer2002}. Assuming that we can reasonably classify each incoming task candidate to one of the K prototypes, from which we use the approximated reward functions to determine which candidate receives the resource unit in each episode. Before we formalize our definitions and notations of the framework, let us go through some of the existing K-armed algorithms and see how they can be useful to the resource allocation problem.

\subsection{The K-armed Bandit Problem}
The problem is formalized by a fixed number of slot gambling machines, each defined by a random variable $X_{i,n}$, for $1\leq i \leq K$ and $n \geq 1$. A sequential $N$ plays of machine $i$ give rewards of $X_{i,1},\,X_{i,2},\dots,\, X_{i,N}$, which are all independent of each other and identically distributed. Based on the sequence of playing history and obtained rewards, one can form an allocation algorithm to pick the next machine. To evaluate the performance of such algorithms, one criterion, the $regret$, is defined as

\begin{equation*}
	regret = \mu^{*}\,n - \mu_j \sum_{j=1}^K E[T_j(n)]
\end{equation*}

where $\mu^{*} \equiv \max_{1\leq i \leq K} \mu_i$, $T_j(n)$ is the number of times machine $j$ has been played during the n plays. Therefore, $regret$ is essentially the expected loss function (opportunity cost) of the played allocation strategy.

\subsection{The Adaptive K-Meteorologist Algorithm}
One just moved to a new city with K TV news channels, what's the optimal strategy to determine the best meteorologist for weather prediction? Diuk et al.

\subsection{Met-Rmax}

\subsection{SCRAM-Rmax}

\subsection{Gittins Index}

\subsection{Thompson Sampling}

%Precisely define the problem you are addressing (i.e. formally specify the inputs and outputs). Elaborate on why this is an interesting and important problem. 

%Describe in reasonable detail the algorithm you are using to address this problem. A psuedocode description of the algorithm you are using is frequently useful. Trace through a concrete example, showing how your algorithm processes this example. The example should be complex enough to illustrate all of the important aspects of the problem but simple enough to be easily understood. If possible, an intuitively meaningful example is better than one with meaningless symbols. 
\section{The Constrained Resource Allocation Framework}

Formally, the Constrained Resource Allocation Framework (CRAF) consists of a sequential task instances, and a set of centralized learning agents who distribute resource unit to the task instances and receive rewards according to the decision they make. The task instances can come in one or more channels $\mathbf{x}_{c,i},\, 1\leq c \leq C,\, 1\leq i \leq capacity(c)$, where $i$ denotes the i-th instance and $c$ denotes the c-th channel.


Instance -> observables
Constraints -> rewards
Prototype
Capacity -> resources available (treat each token as a multi-agent unit)
Queue propagation (only solve the frontier instances at each round)

failure handling


properties:
fairness
progression
safety
liveness

measurements:
latency
throughput
rewards
%What are criteria you are using to evaluate your method? What specific hypotheses does your experiment test? Describe the experimental methodology that you used. What are the dependent and independent variables? What is the training/test data that was used, and why is it realistic or interesting? Exactly what performance data did you collect and how are you presenting and analyzing it? Comparisons to competing methods that address the same problem are particularly useful. 

%Present the quantitative results of your experiments. Graphical data presentation such as graphs and histograms are frequently better than tables. What are the basic differences revealed in the data. Are they statistically significant? 
\section{Experimental Results}
taxi sharing
autonomous intersection management
household welfare

%Answer the following questions for each piece of related work that addresses the same or a similar problem. What is their problem and method? How is your problem and method different? Why is your problem and method better? 
\section{Discussion and Related Work}



%Is your hypothesis supported? What conclusions do the results support about the strengths and weaknesses of your method compared to other methods? How can the results be explained in terms of the underlying properties of the algorithm and/or the data. 

%Briefly summarize the important results and conclusions presented in the paper. What are the most important points illustrated by your work? How will your results improve future research and applications in the area? 
\section{Conclusion}


%What are the major shortcomings of your current method? For each shortcoming, propose additions or enhancements that would help overcome it. 
\section{Future Work}


\section{Acknowledgements}

%Be sure to include a standard, well-formated, comprehensive bibliography with citations from the text referring to previously published papers in the scientific literature that you utilized or are related to your work.
\begin{thebibliography}{1}
\bibitem{dresner2008}
Dresner, K. and Stone, P. A Multiagent Approach to Autonomous Intersection Management. \begin{em}Journal of Artificial Intelligence Research\end{em}, 31:591-656, March 2008.

\bibitem{perkins1999}
Perkins, C. E., and Royer, E. M. (1999, February). Ad-hoc on-demand distance vector routing. In \begin{em}Mobile Computing Systems and Applications, 1999. Proceedings. WMCSA'99. Second IEEE Workshop on (pp. 90-100).\end{em} IEEE.

\bibitem{foster1999}
Foster, I., Kesselman, C., Lee, C., Lindell, B., Nahrstedt, K., and Roy, A. (1999). A distributed resource management architecture that supports advance reservations and co-allocation. In \begin{em}Quality of Service, 1999. IWQoS'99. 1999 Seventh International Workshop on (pp. 27-36).\end{em} IEEE.

\bibitem{baruah1996}
Baruah, S. K., Cohen, N. K., Plaxton, C. G., and Varvel, D. A. (1996). Proportionate progress: A notion of fairness in resource allocation. \begin{em}Algorithmica\end{em}, 15(6), 600-625.

\bibitem{thomas1990}
Thomas, D. (1990). Intra-household resource allocation: An inferential approach. \begin{em}Journal of human resources\end{em}, 635-664.

\bibitem{auer2002}
Auer, P., Cesa-Bianchi, N. and Fischer, P. Finite-time Analysis of the Multiarmed Bandit Problem. In \begin{em} Proc. of 15th International Conference on Machine Learning, pages 100-108\end{em}. Morgan Kaufmann, 1998.

\bibitem{diuk2009}
Diuk, C., Li, L. and Leffler, B. R. The Adaptive k-Meteorologists Problem and Its Application to Structure Learning and Feature Selection in Reinforcement Learning. In \begin{em} Proceedings of the 26th International Conference of Machine Learning\end{em}. Montreal, Canada, 2009. 

\end{thebibliography}




% that's all folks
\end{document}


