\documentclass[conference]{IEEEtran}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{url}
\usepackage[cmex10]{amsmath}
\usepackage{algorithmic}
\usepackage{array}
\usepackage{mdwmath}
\usepackage{mdwtab}
\usepackage{eqparbox}
\usepackage[font=footnotesize]{subfig}
\usepackage{fixltx2e}
\usepackage{setspace}
\usepackage{amssymb}

\begin{document}
\onehalfspacing
\title{A Reinforcement Learning Approach to Constrained Resource Allocation Problems}

\author{\IEEEauthorblockN{C. Vic Hu}
\IEEEauthorblockA{Department of Electrical \& Computer Engineering\\University of Texas at Austin}
}
\maketitle

\begin{abstract}
Resource allocation has been extensively studied in various fields such as wireless communication, intelligent traffic routing, industrial management, parallel architectures and distributed systems. Although there have been many efficient and powerful algorithms proposed in each domain, very few of them are able to address more generalized resource allocation problems on a higher level.
	
This paper proposes a generalized framework and makes three main contributions to solving constrained resource allocation problem using a modified reinforcement learning algorithm. First, we designed a general architecture, known as the Constrained Resource Allocation Framework (CRAF), for the ease of generalized problem mapping and learning. Second, we introduced three properties and three measurements to both quantitatively and qualitatively analyze how well an algorithm performs in CRAF. Finally, we developed two benchmark experiments to demonstrate how a reinforcement algorithm can successfully solve very different resource allocating problems with CRAF.
\end{abstract}

%Motivate and abstractly describe the problem you are addressing and how you are addressing it. What is the problem? Why is it important? What is your basic approach? A short discussion of how it fits into related work in the area is also desirable. Summarize the basic results and conclusions that you will present. 
\section{Introduction}
Resource allocation is an old and widely-solved problem in many fields, including electrical engineering, computer science, economics, management science and many more. It typically involves a fixed number of resource units to be distributed to a set of tasks over a period of time, such as landing aircraft scheduling, wireless communication routing, social security welfare and shared resources in parallel computer architectures. The problems of resource allocating are ubiquitous, but they all essentially boil down to one simple notion--based on a set of criteria, how many resource units should be allocated to which task first, and for how long.

To name a few remarkable research examples to solve this type of problems, Dresner and Stone proposed s reservation system for autonomous intersection management \cite{dresner2008} to allocate right of road for crossing traffics, Perkins and Royer presented a novel routing algorithm for ad-hoc on-demand mobile nodes management \cite{perkins1999}, and Foster et al. came up with a reservation and allocation architecture for heterogeneous resource management in computer network. While they all addressed an elegant solution to a specific domain, most of the techniques cannot be easily transferred to similar decision problems in other domains. In this paper, we focus on addressing exactly this issue and forming a general resource allocation framework that is suitable to be solved by a reinforcement learning method.

The first contribution of this paper is to form the Constrained Resource Allocation Framework (CRAF), in which we designed a generalized architecture to capture most of the resource allocation problems. In addition to the conventional first-come, first-served basis, we introduced the notion of constraints and queue propagation to reflect a more realistic setting and to relax more complicated systems into a single-frontier problem.

Secondly, we defined a collection of analysis criteria and evaluation methods to both qualitatively and quantitatively study how a reinforcement learning algorithm perform on our framework. Lastly, we proposed two benchmark problems to empirically demonstrate how CRAF applies to different resource allocation problems consistently and effectively.

In section 2, we briefly introduce the background knowledge to prepare the general readers to understand our terminologies, including the K-armed bandit problem in reinforcement learning and the clustering algorithms we will compare with. In section 3, we explain the CRAF itself and the intuitions for design and measurements. In section 4, two examples are tested and analyzed. Finally, we make conclusion and propose future improvements in section 5 and 6.


\section{Background}

Unlike the Markov Decision Processes (MDPs) that most of the reinforcement learning algorithms are designed to solve, the notion of state representation, transition functions and reward functions in resource allocation problems can be very complex and challenging to define. Furthermore, the state space for representing the entire global snapshot could be too large to be useful for effective learning. 

Instead of trying to model a resource allocation problem as MDP, we found it more intuitive to formalize it as a K-armed bandit problem, which has been extensively researched in the field of reinforcement learning \cite{auer2002}. Assuming that we can reasonably classify each incoming task candidate to one of the K prototypes, from which we use the approximated reward functions to determine which candidate receives the resource unit in each episode. Before we formalize our definitions and notations of the framework, let us go through some of the existing K-armed algorithms and see how they can be useful to the resource allocation problem.

\subsection{The K-armed Bandit Problem}
The problem is formalized by a fixed number of slot gambling machines, each defined by a random variable $X_{i,n}$, for $1\leq i \leq K$ and $n \geq 1$. A sequential $N$ plays of machine $i$ give rewards of $X_{i,1},\,X_{i,2},\dots,\, X_{i,N}$, which are all independent of each other and identically distributed. Based on the sequence of playing history and obtained rewards, one can form an allocation algorithm to pick the next machine. To evaluate the performance of such algorithms, one criterion, the $regret$, is defined as

\begin{equation*}
	regret = \mu^{*}\,n - \mu_j \sum_{j=1}^K E[T_j(n)]
\end{equation*}

\noindent where $\mu_i$ is the true mean of the generative distribution of $X_i$, and $\mu^{*} \equiv \max_{1\leq i \leq K} \mu_i$, $T_j(n)$ is the number of times machine $j$ has been played during the n plays\cite{auer2002}. Therefore, $regret$ is essentially the expected loss function (opportunity cost) of the played allocation strategy. In reality we don't know the true $\mu_i$ of any slot machines, and thus we need a more empirical method to estimate the payoffs. However, $regret$ gives us an intuition about one of the fundamental problems the reinforcement learning is trying to solve--the dilemma between exploration and exploitation.

\subsection{Balancing Exploration and Exploitation}
As introduced in the previous section, $regret$ is a theoretical index to tell us how close we are from the optimal playing strategy in a K-armed bandit game. To play the optimal action of the game, the learning agent needs to build up a knowledge base of how well each action will pay off. The behavior of trying unknown or low confident actions is called \emph{exploration}. At certain point, the agent may decide it is confident enough to simply play the best action based on the learned knowledge so far, and that playing a suboptimal \emph{exploration} policy is not worthwhile. This idea of greedily choosing whatever is currently the best is known as \emph{exploitation}. It is crucial that a learning agent need to balance both \emph{exploration} and \emph{exploitation} to find the optimal policy in almost any reinforcement learning settings. 

To quantitatively describe how `good' an action is, we can define the action-value function \cite{thathachar1985} prior to time step $t$ to be
\begin{equation*}
	Q_t(a) = \frac{\sum_{i=1}^{N_a}r_i}{N_a}
\end{equation*}
\noindent where $N_a$ is the number of times action $a$ was chosen prior to $t$\cite{sutton1998}. In other words, $Q_t(a)$ is the sampled arithmetic average of the rewards received so far, which approaches to the true action-value $Q^{*}(a)$ as $N_a \rightarrow \infty$. Alternatively, instead of keeping track of the entire history of rewards received $r_i,\, i \in \{1,\dots,N_a\}$ we can simply increment the current action-value toward the new one with a step-size parameter $\alpha$:
\begin{equation*}
	Q_{k+1} = Q_k + \alpha\,[r_{k+1} - Q_k]
\end{equation*}
When we choose $\alpha = \frac{1}{k+1}$, the action-value function falls back to:
\begin{equation*}
	Q_{k+1} = \frac{1}{k+1} \sum_{i=1}^{k+1}r_i
\end{equation*}
\noindent Which is exactly the original sampled average form. To guarantee a  general step-size $\alpha$ such that $\lim_{k\rightarrow \infty}Q_k = Q^*$, it follows that the steps must be large enough to encounter biased initial conditions and randomness, while it cannot be too large for the action-value function to converge eventually. Based on these intuitions, the following conditions must hold according to the stochastic approximation theory in \cite{bertsekas1996}:
\begin{equation*}
	\sum_{k=1}^\infty \alpha_k(a) = \infty \text{ and } \sum_{k=1}^\infty \alpha_k^2(a) < \infty
\end{equation*}

One straightforward policy is to always exploit the greedy action $a^* = \arg\max_{a} Q_t(a)$to maximize the immediate reward at time $t$ . To balance \emph{exploitation} with \emph{exploration}, here are a few alternative methods we will consider:


\-\\
\textbf{$\epsilon$-greedy}\cite{watkins1992}: The simplest way is to explore once in a while and to exploit greedy actions the rest of the time. In other words, we can formalize a policy function as the probability of playing action $a$ at time $t$:
\begin{equation*}
	\pi_t(a) = 
	\begin{cases}
		1-\epsilon, & \text{for }a=a^* = \arg\max_{a} Q_t(a)\\
		\epsilon, & \text{choose any action randomly}
	\end{cases}
\end{equation*}

Although $\epsilon$-greedy method does guarantee $N_a \rightarrow \infty$ as $t \rightarrow \infty$ and thus $Q_t(a) \rightarrow Q^*(a)$ in theory, it is definitely not the most efficient learning strategy in practice, especially the evaluative feedback is non-stationary , which is true in most decision learning processes. However, it is a popular and effective way to balance exploration and exploitation when augmented with more sophisticated algorithms.

\-\\
\textbf{Softmax}\cite{bridle1990}: When exploring suboptimal actions, it makes more sense to choose risky action-values with lower probability, which intuitively blends in a little bit exploitation within the exploration itself. For instance, one may define the policy to be a probabilistic distribution in proportion to the action-values:
\begin{equation*}
	\pi_t(a) = \frac{e^{Q_t(a)/\tau}}{\sum_{\hat{a} \in A}e^{Q_t(\hat{a})/\tau}},\; \tau > 0
\end{equation*}

Notice that as the tuning parameter $\tau \rightarrow 0$, this softmax method is equivalent to the greedy policy, while higher $\tau$ leads to a more uniform action selection.


\-\\
\textbf{Reinforcement Comparison}\cite{sutton1984}: This method is based on a simple idea of ``preferring the good while avoiding the bad.'' Trivial and obvious as it sounds, we need to formalize our definitions of `preference' and `good/bad' to make this concept useful on a multi-armed bandit problem. First, we define the notion of the \emph{reference reward} at time $t$, $\overline{r}_t$:
\begin{equation*}
	\overline{r}_{t+1} = \overline{r}_t + \alpha\,[r_t - \overline{r}_t]
\end{equation*}
\noindent which is almost identical to the action-value function $Q_t$ we defined earlier, except that it is the incremental average of all the received rewards, regardless of the actions taken. Now we have the \emph{reference reward} to tell us what's the reward an average action should receive, we can establish our preference to an action based on its reward compared to the average:
\begin{equation*}
	p_{t+1}(a_t) = p_t(a_t) + \beta\,[r_t - \overline{r}_t],\, \beta > 0
\end{equation*}
\noindent which basically increases or decreases how much we prefer an action $a$ according to its reward, parameterized by another step-size parameter $\beta$. Finally, we can determine our policy just like the softmax method:

\begin{equation*}
	\pi_t(a) = \frac{e^{p_t(a)}}{\sum_{\hat{a} \in A}e^{p_t(\hat{a})}}
\end{equation*}

\-\\
\textbf{Pursuit}\cite{thathachar1985}: The \emph{pursuit} method maintains both the action-value estimates $Q_t(a)$ for choosing the greedy action $a^*_t$ and the policy $\pi_t(a)$, which is updated by incrementing toward one and zero according to the following rule:
\begin{equation*}
	\pi_{t+1}(a) = 
	\begin{cases}
		\pi_{t}(a) + \beta\,\left[1-\pi_{t}(a)\right],& \text{for } a = a^*_{t+1}\\
		\pi_{t}(a) + \beta\,\left[0-\pi_{t}(a)\right],& \text{for } a \neq a^*_{t+1}\\
	\end{cases}
\end{equation*}

\-\\
\textbf{More Advanced Algorithms}: More recently, Auer et al. proposed a new set of more sophisticated algorithms to solve the multi-armed bandit problem in finite time \cite{auer2002}. Here we list a few that we consider to implement on our framework. The details and analysis of these algorithms are beyond the scope of this paper, but we encourage the reader to refer to their original work \cite{auer2002}

\-\\
\textbf{UCB1}: deterministic policy\\
\emph{Initialization}: play each action once\\
\emph{For each iteration}: play action $a^*$
\begin{equation*}
	a^*_t = \arg\max_{a \in A} Q_t(a) + \sqrt{\frac{2\ln N}{N_a}}
\end{equation*}
\noindent where $Q_t(a)$ is computed as the sampled average reward, $N_a$ is the number of $a$ played so far, and $N$ is the total rounds played ($N = \sum_{a \in A} N_a$)

\-\\
\textbf{UCB2}: deterministic policy\\
\emph{Initialization}: Set $r_a = 0\;\forall a \in A$ and play each action once\\
\emph{For each iteration}: 
\begin{enumerate}
	\item Choose 
	\begin{equation*}
		a^* = \arg\max_{a \in A} Q_t(a) + \sqrt{\frac{(1 + \alpha)\ln(e N /\tau(r_a))}{2 \tau(r_a)}}
	\end{equation*}
	\begin{equation*}
		\tau(x) = \lceil(1+\alpha)^x\rceil,\;0<\alpha<1
	\end{equation*}
	\item Play action $a$ exactly $\tau(r_a + 1) - \tau(r_a)$ times
	\item $r_a \leftarrow r_a + 1$
	\item slowly decrease $\alpha$
\end{enumerate}

\-\\
\textbf{$\epsilon_n$-greedy}: randomized policy ($\epsilon$ needs to go to 0)\\
\emph{Initialization}: Define $\epsilon_n \in (0,1],\,n=1,2,\dots$
\begin{equation*}
	\epsilon_n = min\{1, \frac{cK}{d^2n}\},\; c > 0 \text{ and }0 < d < 1 
\end{equation*}
\emph{For $n=1,2,\cdots$}: 
\begin{enumerate}
	\item Let $a_n = \arg\max_{a\in A} Q_n(a)$
	\item Play with action selected according to policy:
	\begin{equation*}
		\pi_n(a) = 
		\begin{cases}
			1 - \epsilon_n, & \text{for } a_n\\
			\epsilon_n, & random
		\end{cases}
	\end{equation*}
\end{enumerate}


\-\\
\textbf{UCB1-NORMAL}: deterministic policy \\
\emph{For $n=1,2,\cdots$}: 
\begin{enumerate}
	\item If $n_a < \lceil8\log{n}\rceil \; \forall a \in A$, play $a$
	\item Otherwise, play
	\begin{equation*}
		a^{*}_n = \arg\max_{a \in A} Q_n(a) + \sqrt{16 \cdot \frac{q_a - n_a Q^2_n(a)}{n_a - 1} \cdot \frac{\ln(n-1)}{n_a}}
	\end{equation*}
	\noindent where $q_a$ is the sum of squared rewards from action $a$ so far.
	\item Update $Q_n(a)$ and $q_a$ with the obtained reward
\end{enumerate}


%
%\-\\
%\textbf{UCB1-TUNED}: deterministic policy\\
%\emph{Initialization}: play each action once\\
%\emph{For each iteration}: play action $a^*$
%\begin{equation*}
%	a^*_t = \arg\max_{a \in A} Q_t(a) + \sqrt{\frac{\ln N}{N_a}\min\{1/4,V_a(N_a)\}}
%\end{equation*}
%
%

\-\\



\subsection{Prototype Classification}


\subsubsection{Distance Functions}
The idea of a particle is essentially a sampled instance of the topic distribution over a time sequence, represented by a vector $\mathbf{w}_i\in \mathbb{R}^{|V|},\; i \in \{1,\cdots,N\}$, where $|V|$ is the total vocabulary size of all the topic words appeared. Since one of our intermediate objectives is to formalize clusters between these particles, we need to first define how we will measure the similarity or distance between any pair of particles $\mathbf{w}_i,\; \mathbf{w}_j$. 

\-\\
\textbf{Minkowski}: 
\begin{equation*}
	d = \sqrt[p]{\sum_{k=1}^{|V|}|w_{ik} - w_{jk}|^p}
\end{equation*}
 Note that when $p=1$, the Minkowski reduces to the city block distance, while $p=2$ gives the Euclidean distance and $p=\infty$ yields the Chebychev distance.

\-\\
\textbf{Cosine}: 
\begin{equation*}
d = 1 - \frac{\mathbf{w}_i\,\mathbf{w}_j^T}{|\mathbf{w}_i|_2|\mathbf{w}_j|_2}
\end{equation*}

\-\\
\textbf{Correlation}: 
\begin{equation*}
d = 1 - \frac{(\mathbf{w}_i - \overline{\mathbf{w}}_i)(\mathbf{w}_j - \overline{\mathbf{w}}_j)^T}{|(\mathbf{w}_i - \overline{\mathbf{w}}_i)|_2|(\mathbf{w}_j - \overline{\mathbf{w}}_j)|_2}
\end{equation*}

where 
\begin{equation*}
	\overline{\mathbf{w}}_i = \frac{1}{|V|}\sum_{k=1}^{|V|}w_{ik},\;
	\overline{\mathbf{w}}_j = \frac{1}{|V|}\sum_{k=1}^{|V|}w_{jk}
\end{equation*}

\-\\
\textbf{Jaccard}: 
\begin{equation*}
	d = \frac{\# \left[(w_{ik} \neq w_{jk})\cap((w_{ik} \neq 0)\cup(w_{jk} \neq 0))\right]}{\#\left[(w_{ik} \neq 0)\cup(w_{jk} \neq 0)\right]}
\end{equation*}

\-\\

\subsubsection{Clustering Algorithms}
LDA is essentially pulling out the `principal components' from the text documents, reducing a large archive of data into just $K$ representative topics $\mathbf{\beta}_{1:K}$. Therefore, our assumption is that by grouping similar particles induced from past topics, we can observe the clustering patterns and make reasonable predictions on how the future topics will be like.

Now we have a collection of well-defined distance functions, we can start looking at clustering methods to group our particles together accordingly, and here are a set of common clustering algorithms we will consider:

\-\\
\textbf{K-Means}\cite{steinhaus1957}: As the name itself suggests, the K-Means clustering algorithm consists of K cluster centroids and moves these means iteratively towards the center of its closest neighbors, until they no longer change. Although K-Means has been proved to be guaranteed for convergence \cite{selim1984}, its clustering performance is often correlated to how the seeds are initialized at the beginning, and the optimal choice of $K$ is often not apparent (in our case, it is the same as the number of topics.)

\begin{verbatim}
1. Initialize the means by picking 
   K samples at random
2. Iterate
2.a. Assign each instance to 
     its nearest mean
2.b. Move the current means to 
	 the center of all the 
	 associated points
\end{verbatim}
\-\\
\textbf{Hierarchical Agglomerative Clustering (HAC):} This algorithm starts with treating every instances as individual cluster, and iteratively joins pairs of similar clusters repeatedly until there is only one. If we take the merging history and form a hierarchical binary tree, it will look like the dendrogram in Fig.~\ref{dendrogram}.

\begin{figure}[h]
	\center	
	\includegraphics[width=0.35\textwidth]{fig/dendrogram.png}
	\caption{A sample dendrogram by HAC}
	\label{dendrogram}
\end{figure}

Although we have defined the distance functions in the previous section, we have yet formalized the similarity functions between clusters. In our method, we will focus on the following four similarity functions:

\begin{itemize}
	\item \textbf{Single-linkage}\cite{sibson1973}: Also known as the nearest neighbor, computes the distance between the two closest elements from two clusters
	\item \textbf{Complete-linkage}\cite{Spath1980}: The conjugate of \textbf{single-linkage}, also known as the farthest neighbor, computes the distance between the two farthest elements (maximum distance) from two clusters
	\item \textbf{UPGMA}\cite{sokal1958} Unweighted Pair Group Method with Averaging calculates the distance between two clusters, $C_i \& C_j$, by averaging all distances between any pair of objects from the two clusters.
	\begin{equation*}
		dist(C_i, C_j) = \frac{1}{|C_i||C_j|}\sum_{w_i \in C_i}\sum_{w_j \in C_j}dist(w_i, w_j)
	\end{equation*}
	Now, let's call this newly-formed cluster $C_{ij}$ and compare its distance with another cluster $C_k$:
	\begin{equation*}
		dist(C_{ij}, C_k) =  \frac{|C_i|dist(c_i,c_k) + |C_j|dist(c_j,c_k)}{|C_i|+|C_j|}
	\end{equation*}
	where $c_i,\,c_j,\,c_k$ are the centroids for clusters $C_i,\;C_j,\;C_k$
	\item \textbf{WPGMA} Weighted Pair Group Method with Averaging is similar to UPGMA except that the cluster distance is now calculated as:
	\begin{equation*}
		dist(C_{ij}, C_k) =  \frac{dist(c_i,c_k) + dist(c_j,c_k)}{2}
	\end{equation*}
\end{itemize}

 The behavior of HAC is often dominated by the chosen similarity function. While each variant has a different set of clustering patterns it is good at capturing, all of them have certain vulnerabilities. 

%
%\subsection{The Adaptive K-Meteorologist Algorithm}
%One just moved to a new city with K TV news channels, what's the optimal strategy to determine the best meteorologist for weather prediction? Diuk et al.
%
%\subsection{Met-Rmax}
%
%\subsection{SCRAM-Rmax}
%
%\subsection{Gittins Index}
%
%\subsection{Thompson Sampling}

%Precisely define the problem you are addressing (i.e. formally specify the inputs and outputs). Elaborate on why this is an interesting and important problem. 

%Describe in reasonable detail the algorithm you are using to address this problem. A psuedocode description of the algorithm you are using is frequently useful. Trace through a concrete example, showing how your algorithm processes this example. The example should be complex enough to illustrate all of the important aspects of the problem but simple enough to be easily understood. If possible, an intuitively meaningful example is better than one with meaningless symbols. 
\section{The Constrained Resource Allocation Framework}

Formally, the Constrained Resource Allocation Framework (CRAF) consists of a sequence of task instances, and a centralized learning agent who distributes resource units to the task instances and receives rewards according to the decision they make. The task instances can come in one or more channels $\mathbf{x}_{c,i},\, 1\leq c \leq C,\, 1\leq i \leq capacity(c)$, where $i$ denotes the i-th instance and $c$ denotes the c-th channel. To transform a resource allocation problem into a learnable framework, the formalization process under CRAF can be broke down into three phases:

\begin{enumerate}
	\item Phase One
	\begin{itemize}
		\item \textbf{Constraint}: The instance $x_{c,i}$ needs a total of $num(x_{c,i})$ resource units before the deadline $exp(x_{c,i})$
		\item \textbf{Objective}: A high-level criteria that the learning agent should achieve, which adds bias against the reward functions
	\end{itemize}
	\item Phase Two
	\begin{itemize}
		\item \textbf{Priority Score}: $ps(x_{c,i})$, a function of both the \textbf{Constraint} and \textbf{Objective}
		\item \textbf{Queue Propagation}: $qp(x_{c,i}) = \gamma qp(x_{c,i+1}) + \tau \,ct(x_{c,i})$ A linear combination of the current constraint and the same function from the next instance in queue
		\item \textbf{Feature Selection}: Use the relevant features to augment the instance $x_{c,i}$ into a feature vector in a higher dimensional space. Domain knowledge may help reduce the dimensionality.
	\end{itemize}
	\item Phase Three
	\begin{itemize}
		\item \textbf{Prototype Classification}: Given a manually-chosen $K$, the current instance $x_{c,i}$ is classified into one of the $K$ clustering prototypes with respect to the two parameters obtained in the previous phase $ps(x_{c,i}),\, qp(x_{c,i})$
		\item \textbf{Reward Function}: The learning agent receives a reward of $r_p+r_o$ if it fulfills the constraint of $x_{c,i}$, or $r_f-r_o$ otherwise
	\end{itemize}
\end{enumerate}

Now we formed the $K$ prototypes, we can have the resource allocating agent learn to choose from the channel frontiers just like in the multi-armed bandit problem. For each round, the agent faces a total of $C$ channels, while each channel has a number of instance tasks waiting to be processed. The agent is given the prototype number, $k$, of the first instance (frontier) of each channel, and it has to choose which frontier to give its resource for that round based on $k$. Unlike the traditional K-armed bandit problem, there are only a subset of $K$ presented at a time if $C < K$. Furthermore, Each channel frontier doesn't necessarily have distinct $k$, which the agent break the tie randomly after deciding which $k$ to choose from the available subset. The pseudo code of this algorithm can be found in Table.~\ref{pseudo_code}.


\begin{table}[h]
%	\centering
	\begin{tabular}{r | p{7.5cm}}
		Step & Instruction\\
		\hline
		\hline
		1 & For each time step $n$\\
		\hline
		2 & $K_n = \{K(x_{1,c}),\,c=1,\dots,C\}$\\
		  & //Observe the prototypes of all the available channel frontiers as the resource candidates\\
		\hline
		3 & $k_* = bandit(K_n)$\\
		  & //Choose the optimal candidate instance to give the resource by solving a k-armed bandit problem \\
		\hline
		4 & Ready for the next round\\
	\end{tabular}
	\caption{Pseudo code for the frontier choosing algorithm in CRAF. }
	\label{pseudo_code}
\end{table}

%
%Instance -> observables
%Constraints -> rewards
%Prototype
%Capacity -> resources available (treat each token as a multi-agent unit)
%Queue propagation (only solve the frontier instances at each round)
%
%failure handling
%
%

To examine how well an algorithm solves a general resource allocation problem, we proposed the following properties:

\begin{itemize}
\item \textbf{Safety}: One resource cannot be shared by more than one task instance at a time.
\item \textbf{Fairness}: Task instances with higher priority scores should always be served first. 
\item \textbf{Progression}: Every task instance should get the resource in bounded and finite time.
\end{itemize}

The algorithm we proposed satisfies the \emph{safety} property since the resource is controlled by a centralized allocating agent, and thus no more than one task instance may share the same resource at any time. However, the \emph{fairness} and \emph{progression} properties of CRAF depend on which K-armed bandit algorithm is chosen.

To quantitatively analyze a resource allocating algorithm, we introduced the following measurement criteria:
\begin{itemize}
\item \textbf{Latency}: The average time each task instance has to wait in line (channel).
\item \textbf{Throughput}: The average number of task instances processed in a period of time.
\item \textbf{Rewards}: The average reward accumulated in a period of time.
\end{itemize}


%measurements:
%latency
%throughput
%rewards


%What are criteria you are using to evaluate your method? What specific hypotheses does your experiment test? Describe the experimental methodology that you used. What are the dependent and independent variables? What is the training/test data that was used, and why is it realistic or interesting? Exactly what performance data did you collect and how are you presenting and analyzing it? Comparisons to competing methods that address the same problem are particularly useful. 

%Present the quantitative results of your experiments. Graphical data presentation such as graphs and histograms are frequently better than tables. What are the basic differences revealed in the data. Are they statistically significant? 
\section{Experimental Results}

\subsection{Candy Distribution}
In this section, we propose an example testing bed to demonstrate how exactly CRAF works in a straightforward problem. Suppose it's the desert break in a kindergarden and there are $C$ lines of children (channel), but only one teacher distributing the candies. There are constantly children coming inside the classroom and waiting in a random line for the candy. Each child has different patience and desire for the candy, which impact how happy they will be when receiving the candy. The teacher is trying to maximize the total amount of happiness in the classroom.

In this problem, we define the i-th child in line $c$ as a task instance, $x_{i,c}$, which has two constraints--one for their desire, $c_1(x_{i,c})$, and the other for their patience, $c_2(x_{i,c})$. In this case, the objective is to maximize children's happiness, which will be embedded in the reward function. That's all for \emph{Phase One}. We've defined our constraints and objective, and now let's pass these variables to the next phase.

In \emph{Phase Two}, we need to map the constraints and objective functions we defined in the previous phase into a set of normalized features for clustering. Here, we introduced two more features--$c_3(x_{i,c}) = \sum_{n = i}^{|C_c|}\#x_{i,c}$, the number of children behind $x_{i,c}$, and $c_4(x_{i,c}) = \sum_{c \in C}\#x_{i,c}$, the number of frontiers competing at the front of every lines. Now we have these two new features in addition to the two we introduced in \emph{Phase One}, we normalize these four parameters and pass them to \emph{Phase Three} for clustering.

Finally in \emph{Phase Three}, we apply one of the clustering algorithms and distance functions we mentioned earlier to group these parameterized instances into $K$ clusters. Furthermore, we define the reward function of choosing channel $m$ to be $R(m) = pr(m) + \sum_{j \neq m}nr(j)$, which consists of a positive reward received from the chosen channel and the negative rewards given by the rest of missed channels. To simplify our model, we define $pr(j) = c_1(x_{i,j}) + c_2(x_{i,j})$ and $nr(j) = -0.1\, c_1(x_{i,j})\times c_2(x_{i,j})$. Note that we intentionally make up a convoluted reward function that is not linear, and leave $c_3,\,c_4$ completely independent to the reward to examine the learning agent's ability to discover this nontrivial pattern. 

In Fig.~\ref{kmeans_dists}, ~\ref{hac_dists}, and ~\ref{hac_links}, we compared how clustering algorithms with different variations impact the outcome of the candy distribution example. For the K-Means algorithm, we experimented with the Minkowski-2 (Euclidean), Minkowski-1 (city block), cosine, and correlation distance functions. As shown in Fig.~\ref{kmeans_dists}, all flavors of the K-Means yield a fairly consistent distribution, with a majority in one big clusters and many smaller ones. For the Hierarchical Agglomerative  Clustering (HAC) algorithms comparisons in Fig.~\ref{hac_dists}, however, we found that the correlation distance function gave us only one cluster, while the others consistently resulted in two comparable ones. For the linkage function comparisons in Fig.~\ref{hac_links}, single linkage method appears to be cleaner and more deterministic about the two-cluster pattern, compared to the other linkage methods. 

\begin{figure}
	\center	
	\includegraphics[width=0.4\textwidth]{fig/kmeans_dists.png}
	\caption{A histogram to compare the effects of using different distance functions in K-Means. The column stands for $K=10$ topics, while each color represents the number of corresponding instances in each channel.}
	\label{kmeans_dists}
\end{figure}

\begin{figure}
	\center	
	\includegraphics[width=0.5\textwidth]{fig/hac_dists.png}
	\caption{A histogram to compare the effects of using different distance functions in HAC. The column stands for $K=10$ topics, while each color represents the number of corresponding instances in each channel.}
	\label{hac_dists}
\end{figure}

\begin{figure}
	\center	
	\includegraphics[width=0.4\textwidth]{fig/hac_links.png}
	\caption{A histogram to compare the effects of using different linkage functions in HAC. The column stands for $K=10$ topics, while each color represents the number of corresponding instances in each channel.}
	\label{hac_links}
\end{figure}

Now we have the clustering results, we can leave the rest to our k-armed bandit algorithms. In Fig.~\ref{sutton_candy_r}, we compared the accumulated rewards obtained from the candy distribution example by the four basic algorithms we introduced earlier--$\epsilon$-greedy, softmax, reinforcement comparison, and pursuit. We can see that the pursuit method generally has a poorer performance than the other three. 

\begin{figure}
	\center	
	\includegraphics[width=0.3\textwidth]{fig/sutton_candy_r.png}
	\caption{The rewards obtained by the $\epsilon$-greedy, Softmax, Reinforcement Comparison, and Pursuit methods on the candy distribution problem}
	\label{sutton_candy_r}
\end{figure}

In Fig.~\ref{sutton_candy_qr}, we demonstrated the estimated action-value, $Q_t(a)$, where the action is equivalent to the chosen $k$. Softmax (top right) has the most smooth estimation, which also shows a lack of exploration. 

\begin{figure}
	\center	
	\includegraphics[width=0.4\textwidth]{fig/sutton_candy_qr.png}
	\caption{The estimated action-value functions obtained by the $\epsilon$-greedy, Softmax, Reinforcement Comparison, and Pursuit methods on the candy distribution problem}
	\label{sutton_candy_qr}
\end{figure}

Now we look at the learning curve obtained with the UCB algorithm family--the UCB1, UCB2, $\epsilon_n$-greedy, and UCB1-Normal algorithms. In Fig.~\ref{ucb_candy_r}, we can observe almost identical learning curves for UCB1 and UCB1-Normal. Note that the learning curve for UCB2 is always zero, which is most likely resulted from an implementation error. The estimated action-value functions are fairly similar among the UCB1, $\epsilon_n$-greedy, and UCB1-Normal methods as shown in Fig.~\ref{ucb_candy_qr}.

\begin{figure}
	\center	
	\includegraphics[width=0.3\textwidth]{fig/ucb_candy_r.png}
	\caption{The rewards obtained by the UCB1, UCB2, $\epsilon_n$-greedy, and UCB1-NORMAL methods on the candy distribution problem}
	\label{ucb_candy_r}
\end{figure}


\begin{figure}
	\center	
	\includegraphics[width=0.4\textwidth]{fig/ucb_candy_qr.png}
	\caption{The estimated action-value functions obtained by the UCB1, UCB2, $\epsilon_n$-greedy, and UCB1-NORMAL methods on the candy distribution problem}
	\label{ucb_candy_qr}
\end{figure}


\subsection{Randomized Test}
Besides the candy distribution testbed, we tested the K-armed bandit algorithms on a separate benchmark to generalize our results. In this testbed, we created $C$ channels of instances with randomly-generated prototype numbers and the corresponding rewards. In this case, the positive reward function, $pr = k + \mathbf{N}(0,0.01)$, is defined to be proportional to the prototype index $k$ with a small gaussian noise. The negative reward function, $nr = - \frac{pr}{C}$, is a scaled negative of $pr$.

In Fig.~\ref{sutton_random_r}, we experimented the four basic algorithms again and found that the pursuit method still underperforms the others. Moreover, we can now see a clear distinction that softmax has the highest accumulated rewards overall, followed by the reinforcement comparison method, and then the $\epsilon$-greedy. 

\begin{figure}
	\center	
	\includegraphics[width=0.3\textwidth]{fig/sutton_random_r.png}
	\caption{The rewards obtained by the $\epsilon$-greedy, Softmax, Reinforcement Comparison, and Pursuit methods on the randomized testbed}
	\label{sutton_random_r}
\end{figure}

The performance in Fig.~\ref{sutton_random_r} can be reasonably explained when we look at the estimated action-value functions in Fig.~\ref{sutton_random_qr}. Softmax has a clean distinction between the action-values but again demonstrated a sign of lack of exploration. On the other hand, we can see a fair amount of exploration for both the $\epsilon$-greedy and pursuit algorithms, which can be demonstrated from their more accurate value estimations. Nonetheless, they don't seem to be able to catch up the optimal strategy quickly enough to compete with softmax.


\begin{figure}
	\center	
	\includegraphics[width=0.4\textwidth]{fig/sutton_random_qr.png}
	\caption{The estimated action-value functions obtained by the $\epsilon$-greedy, Softmax, Reinforcement Comparison, and Pursuit methods on the randomized testbed}
	\label{sutton_candy_qr}
	\label{sutton_random_qr}
\end{figure}

With the UCB algorithms in Fig.~\ref{ucb_random_r}, the differences between the UCB1, UCB-Normal, and $\epsilon_n$-greedy methods become more clear as well; we can see that UCB1 quickly picked up the optimal strategy and stayed there, while UCB1-Normal and $\epsilon_n$-greedy seems to spend more time exploring other options. We can see that UCB1-Normal does have an uprising learning curve during its exploration, which demonstrates its ability to effectively balance exploitation and exploration.

\begin{figure}
	\center	
	\includegraphics[width=0.3\textwidth]{fig/ucb_random_r.png}
	\caption{The rewards obtained by the UCB1, UCB2, $\epsilon_n$-greedy, and UCB1-NORMAL methods on the randomized testbed}
	\label{ucb_random_r}
\end{figure}

In Fig.~\ref{ucb_random_qr}, we can see that UCB1 shows a monotone pattern on the high action-values, which is similar to the softmax in Fig.~\ref{sutton_random_qr}. The $\epsilon_n$-greedy and UCB1-Normal have a relatively better estimation due to their ability to balance exploitation with exploration.

Overall, we observed a similar correlation between the learning curve of a k-armed bandit algorithm and its ability to correctly estimate true action-value functions in both the candy distribution and random testbeds.

\begin{figure}
	\center	
	\includegraphics[width=0.4\textwidth]{fig/ucb_random_qr.png}
	\caption{The estimated action-value functions obtained by the UCB1, UCB2, $\epsilon_n$-greedy, and UCB1-NORMAL methods on the randomized testbed}
	\label{ucb_random_qr}
\end{figure}

\newpage

%1. 2-way intersection (simplest form)
%2. 4-way intersection (special corner cases)
%3. n-children candy distribution (multi-channel)
%taxi sharing

%autonomous intersection management
%household welfare

%Answer the following questions for each piece of related work that addresses the same or a similar problem. What is their problem and method? How is your problem and method different? Why is your problem and method better? 
%\section{Discussion and Related Work}



%Is your hypothesis supported? What conclusions do the results support about the strengths and weaknesses of your method compared to other methods? How can the results be explained in terms of the underlying properties of the algorithm and/or the data. 

%Briefly summarize the important results and conclusions presented in the paper. What are the most important points illustrated by your work? How will your results improve future research and applications in the area? 
\section{Conclusion}
In this paper, we proposed a generalized resource allocation framework to be solved as a classic K-armed bandit problem in reinforcement learning. We demonstrated two simple benchmark domains and compared the results learned by a collection of K-armed bandit algorithms. Our experimental results reinforced the relations of exploitation and exploration tradeoff, and fortified a new concept to solve resource allocation problem in the reinforcement learning fashion.

%What are the major shortcomings of your current method? For each shortcoming, propose additions or enhancements that would help overcome it. 
\section{Future Work}
We introduced a selection of concepts to quantitatively measure the efficiency and performance of a resource allocating algorithm, including latency, throughput, and rewards. However, we haven't given a concrete example and analysis on how this can be achieved in practice, which left a lot of room for improvements upon this project. Moreover, we've only demonstrated two artificial problems with intentional setups for testing. The value of CRAF could be significantly enhanced if a more practical domain problem is tested and shows promising results.

\section{Acknowledgements}
We really appreciate Professor Stone's knowledgable guidance and tremendous patience with this project. This work will not be possible without him.

\newpage
%Be sure to include a standard, well-formated, comprehensive bibliography with citations from the text referring to previously published papers in the scientific literature that you utilized or are related to your work.
\begin{thebibliography}{1}

\bibitem{sutton1998}
Sutton, R. S., and Barto, A. G. (1998). \emph{Introduction to reinforcement learning}. MIT Press.

\bibitem{dresner2008}
Dresner, K. and Stone, P. A Multiagent Approach to Autonomous Intersection Management. \begin{em}Journal of Artificial Intelligence Research\end{em}, 31:591-656, March 2008.

\bibitem{perkins1999}
Perkins, C. E., and Royer, E. M. (1999, February). Ad-hoc on-demand distance vector routing. In \begin{em}Mobile Computing Systems and Applications, 1999. Proceedings. WMCSA'99. Second IEEE Workshop on (pp. 90-100).\end{em} IEEE.

\bibitem{foster1999}
Foster, I., Kesselman, C., Lee, C., Lindell, B., Nahrstedt, K., and Roy, A. (1999). A distributed resource management architecture that supports advance reservations and co-allocation. In \begin{em}Quality of Service, 1999. IWQoS'99. 1999 Seventh International Workshop on (pp. 27-36).\end{em} IEEE.

\bibitem{baruah1996}
Baruah, S. K., Cohen, N. K., Plaxton, C. G., and Varvel, D. A. (1996). Proportionate progress: A notion of fairness in resource allocation. \begin{em}Algorithmica\end{em}, 15(6), 600-625.

\bibitem{thomas1990}
Thomas, D. (1990). Intra-household resource allocation: An inferential approach. \begin{em}Journal of human resources\end{em}, 635-664.

\bibitem{auer2002}
Auer, P., Cesa-Bianchi, N. and Fischer, P. Finite-time Analysis of the Multiarmed Bandit Problem. In \begin{em} Proc. of 15th International Conference on Machine Learning, pages 100-108\end{em}. Morgan Kaufmann, 1998.

\bibitem{diuk2009}
Diuk, C., Li, L. and Leffler, B. R. The Adaptive k-Meteorologists Problem and Its Application to Structure Learning and Feature Selection in Reinforcement Learning. In \begin{em} Proceedings of the 26th International Conference of Machine Learning\end{em}. Montreal, Canada, 2009. 

\bibitem{bertsekas1996}
Bertsekas, D. P., and Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming (Optimization and Neural Computation Series, 3). \begin{em}Athena Scientific\end{em}, 7, 15-23.

\bibitem{thathachar1985}
Thathachar, M. A. L., and Sastry, P. S. (1985). \emph{A new approach to the design of reinforcement schemes for learning automata. Systems, Man and Cybernetics}, IEEE Transactions on, (1), 168-175.

\bibitem{watkins1992}
Watkins, C. J., and Dayan, P. (1992). Q-learning. \emph{Machine learning}, 8(3-4), 279-292.

\bibitem{bridle1990}
Bridle, J. S. (1990). Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. In \emph{Neurocomputing} (pp. 227-236). Springer Berlin Heidelberg.

\bibitem{sutton1984}
Sutton, R. S. (1984). Temporal credit assignment in reinforcement learning.

\bibitem{steinhaus1957}
Steinhaus, H. (1957). Sur la division des corps matériels en parties. Bull. Acad. Polon. Sci. (in French) 4 (12): 801–804. MR 0090073. Zbl 0079.16403.

\bibitem{sibson1973}
 R. Sibson (1973). SLINK: an optimally efficient algorithm for the single-link cluster method. \emph{The Computer Journal (British Computer Society)} 16 (1): 30–34.
 
\bibitem{Spath1980}
H. Späth (1980). Cluster Analysis Algorithms.

\bibitem{sokal1958}
Sokal R and Michener C (1958). A statistical method for evaluating systematic relationships. \emph{University of Kansas Science Bulletin 38: 1409–1438.}

\end{thebibliography}




% that's all folks
\end{document}


